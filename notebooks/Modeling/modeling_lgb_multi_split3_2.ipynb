{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e52f566f-d9d0-43ab-b6b1-65a3961a1cd4",
   "metadata": {},
   "source": [
    "# Modeling - XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4cac22fe-1f45-499a-afe2-dcc2c0d42aaf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.append('../../src')\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import pickle\n",
    "from models.lightgbm_model import LightGBModel\n",
    "from tuning import random_search_tuning\n",
    "from utils import  generate_combinations, generate_dataset_split, save_combination\n",
    "from itertools import combinations\n",
    "from lightgbm import LGBMClassifier\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f373d6d-f13a-4e54-82dd-b56858bbaea0",
   "metadata": {},
   "source": [
    "## Parameters for tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "be5acff9",
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = {\n",
    "    'num_leaves':  np.arange(20, 50, 10),\n",
    "    'learning_rate': np.arange(0.001, 0.1, 0.005),\n",
    "    'max_depth': np.arange(2, 8),\n",
    "    'min_data_in_leaf': np.arange(10, 30, 5),\n",
    "    'min_gain_to_split': np.arange(0, 0.05, 0.01),\n",
    "    'n_estimators': np.arange(50, 150, 10),\n",
    "    'colsample_bytree': np.arange(0.6, 1.0, 0.1),\n",
    "    'early_stopping_rounds': [5],\n",
    "    'objective': ['multiclassova'], \n",
    "    'num_class': [3],\n",
    "    'metric':['auc_mu'],\n",
    "    'device': ['gpu'],\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd493441",
   "metadata": {},
   "source": [
    "## Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ef26aa46",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "cities_siglas = {\n",
    "    \"A\": \"Porto Alegre\",\n",
    "    \"B\": \"Marabá\",\n",
    "    \"C\": \"Brasília\",\n",
    "    \"D\": \"Belo Horizonte\",\n",
    "    \"E\": \"Juazeiro do Norte\",\n",
    "    \"F\": \"Recife\"\n",
    "}\n",
    "\n",
    "cities = [\"Porto Alegre\", \"Marabá\", \"Brasília\", \"Belo Horizonte\", \"Juazeiro do Norte\", \"Recife\"]\n",
    "\n",
    "polos_sigla = ['A', 'B', 'C', 'D', 'E', 'F']\n",
    "polos = [cities_siglas[s] for s in polos_sigla]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a4d998c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "splits = []\n",
    "for i in range(0, len(polos_sigla)):\n",
    "    splits.append(generate_combinations(polos[:i] + polos[i+1:], 3, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "90194cad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combinação 0: [['Marabá', 'Brasília', 'Belo Horizonte'], ['Juazeiro do Norte', 'Recife']] training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\caior\\OneDrive\\Documentos\\GitHub\\xai-nui-classification\\venv\\Lib\\site-packages\\joblib\\externals\\loky\\backend\\context.py:136: UserWarning: Could not find the number of physical cores for the following reason:\n",
      "found 0 physical cores < 1\n",
      "Returning the number of logical cores instead. You can silence this warning by setting LOKY_MAX_CPU_COUNT to the number of cores you want to use.\n",
      "  warnings.warn(\n",
      "  File \"c:\\Users\\caior\\OneDrive\\Documentos\\GitHub\\xai-nui-classification\\venv\\Lib\\site-packages\\joblib\\externals\\loky\\backend\\context.py\", line 282, in _count_physical_cores\n",
      "    raise ValueError(f\"found {cpu_count_physical} physical cores < 1\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Unknown parameter: eval_metric\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=15, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=15\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.0, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.0\n",
      "[LightGBM] [Warning] early_stopping_round is set=5, early_stopping_rounds=5 will be ignored. Current value: early_stopping_round=5\n",
      "[LightGBM] [Warning] Unknown parameter: eval_metric\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=15, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=15\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.0, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.0\n",
      "[LightGBM] [Info] Number of positive: 265926, number of negative: 48446\n",
      "[LightGBM] [Info] Number of positive: 12137, number of negative: 302235\n",
      "[LightGBM] [Info] Number of positive: 36309, number of negative: 278063\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 16646\n",
      "[LightGBM] [Info] Number of data points in the train set: 314372, number of used features: 70\n",
      "[LightGBM] [Info] Using GPU Device: Intel(R) Iris(R) Xe Graphics, Vendor: Intel(R) Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 49 dense feature groups (15.59 MB) transferred to GPU in 0.039566 secs. 1 sparse feature groups\n",
      "[LightGBM] [Warning] Unknown parameter: eval_metric\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=15, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=15\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.0, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.0\n",
      "[LightGBM] [Warning] early_stopping_round is set=5, early_stopping_rounds=5 will be ignored. Current value: early_stopping_round=5\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.845896 -> initscore=1.702768\n",
      "[LightGBM] [Info] Start training from score 1.702768\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.038607 -> initscore=-3.214946\n",
      "[LightGBM] [Info] Start training from score -3.214946\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.115497 -> initscore=-2.035782\n",
      "[LightGBM] [Info] Start training from score -2.035782\n",
      "Training until validation scores don't improve for 5 rounds\n",
      "Early stopping, best iteration is:\n",
      "[5]\tvalid_0's multi_logloss: 0.598661\n",
      "[LightGBM] [Warning] Unknown parameter: eval_metric\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=15, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=15\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.0, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.0\n",
      "[LightGBM] [Warning] Unknown parameter: eval_metric\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=15, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=15\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.0, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.0\n",
      "[LightGBM] [Warning] early_stopping_round is set=5, early_stopping_rounds=5 will be ignored. Current value: early_stopping_round=5\n",
      "[LightGBM] [Warning] Unknown parameter: eval_metric\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=15, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=15\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.0, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.0\n",
      "[LightGBM] [Info] Number of positive: 265926, number of negative: 48446\n",
      "[LightGBM] [Info] Number of positive: 12137, number of negative: 302235\n",
      "[LightGBM] [Info] Number of positive: 36309, number of negative: 278063\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 16901\n",
      "[LightGBM] [Info] Number of data points in the train set: 314372, number of used features: 70\n",
      "[LightGBM] [Info] Using GPU Device: Intel(R) Iris(R) Xe Graphics, Vendor: Intel(R) Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 50 dense feature groups (15.59 MB) transferred to GPU in 0.035229 secs. 1 sparse feature groups\n",
      "[LightGBM] [Warning] Unknown parameter: eval_metric\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=15, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=15\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.0, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.0\n",
      "[LightGBM] [Warning] early_stopping_round is set=5, early_stopping_rounds=5 will be ignored. Current value: early_stopping_round=5\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.845896 -> initscore=1.702768\n",
      "[LightGBM] [Info] Start training from score 1.702768\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.038607 -> initscore=-3.214946\n",
      "[LightGBM] [Info] Start training from score -3.214946\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.115497 -> initscore=-2.035782\n",
      "[LightGBM] [Info] Start training from score -2.035782\n",
      "Training until validation scores don't improve for 5 rounds\n",
      "Early stopping, best iteration is:\n",
      "[19]\tvalid_0's multi_logloss: 0.545547\n",
      "[LightGBM] [Warning] Unknown parameter: eval_metric\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=15, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=15\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.0, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.0\n",
      "[LightGBM] [Warning] Unknown parameter: eval_metric\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=15, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=15\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.0, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.0\n",
      "[LightGBM] [Warning] early_stopping_round is set=5, early_stopping_rounds=5 will be ignored. Current value: early_stopping_round=5\n",
      "[LightGBM] [Warning] Unknown parameter: eval_metric\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=15, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=15\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.0, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.0\n",
      "[LightGBM] [Info] Number of positive: 265926, number of negative: 48446\n",
      "[LightGBM] [Info] Number of positive: 12137, number of negative: 302235\n",
      "[LightGBM] [Info] Number of positive: 36309, number of negative: 278063\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 16909\n",
      "[LightGBM] [Info] Number of data points in the train set: 314372, number of used features: 70\n",
      "[LightGBM] [Info] Using GPU Device: Intel(R) Iris(R) Xe Graphics, Vendor: Intel(R) Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 50 dense feature groups (15.59 MB) transferred to GPU in 0.054349 secs. 1 sparse feature groups\n",
      "[LightGBM] [Warning] Unknown parameter: eval_metric\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=15, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=15\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.0, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.0\n",
      "[LightGBM] [Warning] early_stopping_round is set=5, early_stopping_rounds=5 will be ignored. Current value: early_stopping_round=5\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.845896 -> initscore=1.702768\n",
      "[LightGBM] [Info] Start training from score 1.702768\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.038607 -> initscore=-3.214946\n",
      "[LightGBM] [Info] Start training from score -3.214946\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.115497 -> initscore=-2.035782\n",
      "[LightGBM] [Info] Start training from score -2.035782\n"
     ]
    }
   ],
   "source": [
    "\n",
    "models = []\n",
    "save_combination(f'split3_2/{cities[i]}', \"multiclass\", splits[0])\n",
    "for idx, combination in enumerate(splits[0], start=0):\n",
    "    print(f\"Combinação {idx}: {combination} training\")\n",
    "    X_train, y_train = generate_dataset_split(combination[0], \"multiclass\")\n",
    "    X_val, y_val = generate_dataset_split(combination[1], \"multiclass\")\n",
    "    clf = LGBMClassifier()\n",
    "    model = random_search_tuning(clf, parameters)\n",
    "    model = model.fit(X_train, y_train, eval_set=[(X_val, y_val)])\n",
    "    print(model.best_estimator_)\n",
    "    models.append(model)\n",
    "    break\n",
    "for j in range(0, len(models)):\n",
    "    pickle.dump(models[j], open(f\"../../data/models/multiclass/split3_2/{cities[i]}/xgb_{j}.sav\", 'wb'))\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
